{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeywordsExtractionBERT_Custom.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTWrHQWbyJZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b20847d-e44f-4572-eb47-ad7c95771d12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFdIf9eauB9L"
      },
      "source": [
        "data_path = \"/content/gdrive/My Drive/Data\" \n",
        "model_path = \"/content/gdrive/My Drive/Models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VYbcSZmHeYk"
      },
      "source": [
        "train_path = data_path + \"/maui-semeval2010-train\"\n",
        "test_path = data_path + \"/maui-semeval2010-test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preEPbz6tMpW"
      },
      "source": [
        "\n",
        "\n",
        "> Dataset creation (skip it)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEy72vFZ7AXg"
      },
      "source": [
        "import os\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sacsb0L-0-G",
        "outputId": "8aad55e5-59b9-4b81-e36e-b28c2d9bdcf2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXZCkABQ7Eb-"
      },
      "source": [
        "txt = sorted([f for f in os.listdir(train_path) if f.endswith(\".txt\")]) # all .txt files for training\n",
        "key = sorted([f for f in os.listdir(train_path) if f.endswith(\".key\")]) # all .key files for training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiuOTuCfESRv"
      },
      "source": [
        "test_txt = sorted([f for f in os.listdir(test_path) if f.endswith(\".txt\")]) # all .txt files for testing\n",
        "test_key = sorted([f for f in os.listdir(test_path) if f.endswith(\".key\")]) # all .key files for testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reqj_iYp7J9n"
      },
      "source": [
        "key2txt = dict() # dictionary with (key, txt) pairs\n",
        "for i in range(len(txt)):\n",
        "  key2txt[key[i]] = txt[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln3v09XeEdbv"
      },
      "source": [
        "test_key2txt = dict() # dictionary with (key, txt) pairs\n",
        "for i in range(len(test_txt)):\n",
        "  test_key2txt[test_key[i]] = test_txt[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHRVRO4YHl1B"
      },
      "source": [
        "def kt2st(key, txt, path): # key, txt -> snts - sentences with keywords, tags - tags for each word\n",
        "  lines = \"\"\n",
        "  for line in open(path + \"/\" + txt, 'r'):\n",
        "    lines += \" \" + line.strip()\n",
        "  lines = lines.strip()\n",
        "\n",
        "  tkns = sent_tokenize(lines)\n",
        "  keys = sorted([line.strip() for line in open(path + \"/\" + key, 'r')], key=len, reverse=True)\n",
        "  \n",
        "  snts = []\n",
        "  tags = []\n",
        "  \n",
        "  for t in tkns:\n",
        "    tlist = t.lower().split()\n",
        "    temp = ['O'] * len(tlist)\n",
        "    for k in keys:\n",
        "      if k.lower() in t.lower():\n",
        "        klist = k.lower().split()\n",
        "        if klist[0] in tlist:\n",
        "          i0 = tlist.index(klist[0])\n",
        "          if temp[i0] == 'O':\n",
        "            temp[i0] = 'B'\n",
        "            if len(klist) > 1 and len(tlist) >= i0 + len(klist):\n",
        "              isWrong = False\n",
        "              for i in range(i0 + 1, i0 + len(klist)):\n",
        "                if tlist[i] == klist[i - i0]:\n",
        "                  temp[i] = 'I'\n",
        "                else:\n",
        "                  isWrong = True\n",
        "                  break\n",
        "              if isWrong:\n",
        "                for i in range(i0, i0 + len(klist)):\n",
        "                  temp[i] = 'O'\n",
        "    \n",
        "    if set(temp) != {'O'}:\n",
        "      snts.append(t)\n",
        "      tags.append(temp)\n",
        "  return snts, tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyJDCWzCrtag"
      },
      "source": [
        "temp_snts = []\n",
        "temp_tags = []\n",
        "for key_, txt_ in key2txt.items():\n",
        "  snts_, tags_ = kt2st(key_, txt_, train_path)\n",
        "  temp_snts.append(snts_)\n",
        "  temp_tags.append(tags_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH0e1nNWEoBN"
      },
      "source": [
        "test_temp_snts = []\n",
        "test_temp_tags = []\n",
        "for key_, txt_ in test_key2txt.items():\n",
        "  snts_, tags_ = kt2st(key_, txt_, test_path)\n",
        "  test_temp_snts.append(snts_)\n",
        "  test_temp_tags.append(tags_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p584QvTCsV1F"
      },
      "source": [
        "snts = [item for sublist in temp_snts for item in sublist]\n",
        "tags = [item for sublist in temp_tags for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrixugHvE9jp"
      },
      "source": [
        "test_snts = [item for sublist in test_temp_snts for item in sublist]\n",
        "test_tags = [item for sublist in test_temp_tags for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAH_x219tbAY"
      },
      "source": [
        "\n",
        "\n",
        "> Dataset saving (skip it)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dydq0wywtehY"
      },
      "source": [
        "snts_file = open(data_path + \"/snts.txt\", 'w')\n",
        "for s in snts:\n",
        "  snts_file.write(s + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9NqIBj3wMrv"
      },
      "source": [
        "tags_file = open(data_path + \"/tags.txt\", 'w')\n",
        "for t in tags:\n",
        "  tstr = \"\"\n",
        "  for subt in t:\n",
        "    tstr += str(subt)\n",
        "  tags_file.write(tstr + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOv7VbgyFQuI"
      },
      "source": [
        "test_snts_file = open(data_path + \"/test_snts.txt\", 'w')\n",
        "for s in test_snts:\n",
        "  test_snts_file.write(s + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2f795mxFZTY"
      },
      "source": [
        "test_tags_file = open(data_path + \"/test_tags.txt\", 'w')\n",
        "for t in test_tags:\n",
        "  tstr = \"\"\n",
        "  for subt in t:\n",
        "    tstr += str(subt)\n",
        "  test_tags_file.write(tstr + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1M0t7GptU8K"
      },
      "source": [
        "\n",
        "> Dataset loading (start from here)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn4DGU4ZwGbZ"
      },
      "source": [
        "snts = []\n",
        "tags = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzkpL1KgwmXQ"
      },
      "source": [
        "snts_file = open(data_path + \"/snts.txt\", 'r')\n",
        "for line in snts_file:\n",
        "  snts.append(line.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWuS-JQbwnKk"
      },
      "source": [
        "tags_file = open(data_path + \"/tags.txt\", 'r')\n",
        "count = 0\n",
        "for line in tags_file:\n",
        "  t = []\n",
        "  tstr = line.strip()\n",
        "  for subt in tstr:\n",
        "    t.append(subt)\n",
        "  count += 1\n",
        "  tags.append(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk-BmMQPFhxo"
      },
      "source": [
        "test_snts = []\n",
        "test_tags = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5gGYQtTFjwr"
      },
      "source": [
        "test_snts_file = open(data_path + \"/test_snts.txt\", 'r')\n",
        "for line in test_snts_file:\n",
        "  test_snts.append(line.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqXa060XFoff"
      },
      "source": [
        "test_tags_file = open(data_path + \"/test_tags.txt\", 'r')\n",
        "count = 0\n",
        "for line in test_tags_file:\n",
        "  t = []\n",
        "  tstr = line.strip()\n",
        "  for subt in tstr:\n",
        "    t.append(subt)\n",
        "  count += 1\n",
        "  test_tags.append(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2rN_7yOyMRI"
      },
      "source": [
        "\n",
        "\n",
        "> BERT model creation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khyOG4DHtFKh",
        "outputId": "a830d112-2f08-49a2-a41c-1524ad75a86c"
      },
      "source": [
        "pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 14.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 12.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 12.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/a1/d8b9be4f3996265cb4e42dcd0ba72d61d68062ed6d8ce7e26b37cc399455/boto3-1.16.24-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 24.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.7.0+cu101)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/16/8afd6474045f41bd51002e65862e2066ea2c5de34d0a942e1c4e86098d9a/botocore-1.19.24-py2.py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 11.6MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.24->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.24->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.24 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.24 botocore-1.19.24 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlwmQo6syf3t"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMpmxQUcMiWj"
      },
      "source": [
        "\n",
        "\n",
        "> Hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbYbKdCmy8eN"
      },
      "source": [
        "MAX_LEN = 75\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MGIglJ4zBKO"
      },
      "source": [
        "tag2idx = {'B': 2, 'I': 1, 'O': 0}\n",
        "idx2tag = {2: 'B', 1 : 'I', 0: 'O'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBofj13AzEb2"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA12TCOEMm5u"
      },
      "source": [
        "\n",
        "\n",
        "> Preparing dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S3hDcGFzSJJ",
        "outputId": "23365847-a246-4c75-c081-b1abae39a6a6"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 24651931.51B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h_VokKozVBv"
      },
      "source": [
        "token_snts = [tokenizer.tokenize(snt) for snt in snts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTOaU5N8yr63"
      },
      "source": [
        "token_test_snts = [tokenizer.tokenize(snt) for snt in test_snts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJFd6OYA27nr",
        "outputId": "6b5c0165-a24a-4a1f-a888-c6dc993addb5"
      },
      "source": [
        "inp_ids = pad_sequences([tokenizer.convert_tokens_to_ids(token_snt) for token_snt in token_snts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "out_tgs = pad_sequences([[tag2idx[t] for t in tag] for tag in tags], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (770 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1001 > 512). Running this sequence through BERT will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWYXqxUWy2Da",
        "outputId": "c63bf445-0e0c-41f1-8b15-8c56c3d3a7a2"
      },
      "source": [
        "test_inp_ids = pad_sequences([tokenizer.convert_tokens_to_ids(token_snt) for token_snt in token_test_snts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "test_out_tgs = pad_sequences([[tag2idx[t] for t in tag] for tag in test_tags], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6L8uV_e67Ic"
      },
      "source": [
        "att_masks = [[float(inp_id > 0) for inp_id in inp_idx] for inp_idx in inp_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC0JxNTi-TOm"
      },
      "source": [
        "#att_masks = [[(0.0 if i == 0 else (0.99999 if i > 0 and t == 0 else 1.0)) for i, t in zip(inp_id, out_tg)] for inp_id, out_tg in zip(inp_ids, out_tgs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1afWAJ1w8TqH"
      },
      "source": [
        "train_inp, val_inp, train_out, val_out = train_test_split(inp_ids, out_tgs, random_state=0, test_size=0.1)\n",
        "train_masks, val_masks, _, _ = train_test_split(att_masks, inp_ids, random_state=0, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FIcLGT_9Whf"
      },
      "source": [
        "train_inp = torch.tensor(train_inp)\n",
        "val_inp = torch.tensor(val_inp)\n",
        "train_out = torch.tensor(train_out)\n",
        "val_out = torch.tensor(val_out)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2HH7E4M9wUn"
      },
      "source": [
        "train_data = TensorDataset(train_inp, train_masks, train_out)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-VLZGMhKFmG"
      },
      "source": [
        "val_data = TensorDataset(val_inp, val_masks, val_out)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "val_loader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKxVCjYCzAML"
      },
      "source": [
        "test_att_masks = [[float(inp_id > 0) for inp_id in inp_idx] for inp_idx in test_inp_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrLjg9rXzF6H"
      },
      "source": [
        "test_inp = torch.tensor(test_inp_ids)\n",
        "test_out = torch.tensor(test_out_tgs)\n",
        "test_masks = torch.tensor(test_att_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpE1h-K8zR9b"
      },
      "source": [
        "test_data = TensorDataset(test_inp, test_masks, test_out)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAg4xxmqND3F"
      },
      "source": [
        "BERT model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqb3oDreKctR"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=(len(tag2idx)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoRE_yQaK2VH"
      },
      "source": [
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y39w3pGHQ9-j"
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOLFeqQHRAR9"
      },
      "source": [
        "def accuracy(preds, outs):\n",
        "  preds_flat = preds.flatten()\n",
        "  outs_flat = outs.flatten()\n",
        "  return np.sum(preds_flat == outs_flat) / len(outs_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC53vwd_S68d"
      },
      "source": [
        "def train(epochs, max_norm, params, lr, name, isStorable):\n",
        "  optimizer = Adam(params, lr=lr)\n",
        "  best = float('inf')\n",
        "  for i in trange(epochs, desc='Epoch'):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_steps = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "      batch = tuple(b.to(device) for b in batch)\n",
        "      b_inp_ids, b_mask, b_out_tgs = batch\n",
        "\n",
        "      loss = model(b_inp_ids, token_type_ids=None, attention_mask=b_mask, labels=b_out_tgs)\n",
        "      loss.backward()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      train_steps += 1\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_norm)\n",
        "\n",
        "      optimizer.step()\n",
        "      model.zero_grad()\n",
        "    \n",
        "    train_loss /= train_steps\n",
        "    print('Train loss:', train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    eval_loss, eval_acc = 0, 0\n",
        "    eval_steps = 0\n",
        "    preds, outs = [], []\n",
        "\n",
        "    for batch in val_loader:\n",
        "      batch = tuple(b.to(device) for b in batch)\n",
        "      b_inp_ids, b_mask, b_out_tgs = batch\n",
        "\n",
        "      with torch.no_grad():\n",
        "        loss = model(b_inp_ids, token_type_ids=None, attention_mask=b_mask, labels=b_out_tgs)\n",
        "        logits = model(b_inp_ids, token_type_ids=None, attention_mask=b_mask)\n",
        "      \n",
        "      temp_preds = np.argmax(logits.cpu().numpy(), axis=2)\n",
        "      temp_outs = b_out_tgs.cpu().numpy()\n",
        "      preds.extend([list(p) for p in temp_preds])\n",
        "      outs.extend([list(o) for o in temp_outs])\n",
        "\n",
        "      acc = accuracy(temp_preds, temp_outs)\n",
        "      \n",
        "      eval_loss += loss.item()\n",
        "      eval_acc += acc\n",
        "      eval_steps += 1\n",
        "\n",
        "    eval_loss /= eval_steps\n",
        "    eval_acc /= eval_steps\n",
        "    print('Val loss:', eval_loss)\n",
        "    print('Val acc:', eval_acc)\n",
        "    print('F1-Score:', f1_score([oi for o in outs for oi in o], [pi for p in preds for pi in p], average='weighted'))\n",
        "\n",
        "    if isStorable and eval_loss < best:\n",
        "      best = eval_loss\n",
        "      torch.save(model.state_dict(), model_path + \"/key_extractor\" + name + \".pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqD7noD_zerY"
      },
      "source": [
        "\n",
        "> Don't run!!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC78sCWgKRFD"
      },
      "source": [
        "params = [{\"params\": [p for n, p in list(model.classifier.named_parameters())]}] # partially fine-tuning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhGL94lH_G41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1322c501-b2c7-4537-e7a4-3fd8fc33b961"
      },
      "source": [
        "train(epochs=5, max_norm=1.0, params=params, lr=3e-5, name='3e5', isStorable=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.543037089398533\n",
            "Val loss: 0.29869846473721895\n",
            "Val acc: 0.9763753770739068\n",
            "F1-Score: 0.9646194901634687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 1/5 [02:27<09:49, 147.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.2670290276556712\n",
            "Val loss: 0.2504289255422704\n",
            "Val acc: 0.9763103318250375\n",
            "F1-Score: 0.9646319612472058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [05:02<07:29, 149.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.24167041673612752\n",
            "Val loss: 0.24053612176109762\n",
            "Val acc: 0.9763461538461539\n",
            "F1-Score: 0.9646319612472058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [07:40<05:04, 152.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.23376583978781273\n",
            "Val loss: 0.23493403415469563\n",
            "Val acc: 0.9761849547511313\n",
            "F1-Score: 0.9646319612472058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [10:19<02:34, 154.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.228578400473262\n",
            "Val loss: 0.227884577915949\n",
            "Val acc: 0.9763282428355957\n",
            "F1-Score: 0.9646319612472058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 5/5 [12:58<00:00, 155.71s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqGSpMm-M7W5"
      },
      "source": [
        "params = [{\"params\": [p for n, p in list(model.named_parameters())], 'weight_decay_rate': 0.01}] # fully fine-tuning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sp9sbTz0TQ5",
        "outputId": "d3a5ec52-2817-4262-9e13-b9623360435f"
      },
      "source": [
        "train(epochs=5, max_norm=1.0, params=params, lr=3e-5, name='3e5_fully', isStorable=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.18764582382781164\n",
            "Val loss: 0.15021813441725337\n",
            "Val acc: 0.9559568250377074\n",
            "F1-Score: 0.9599754992778731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  20%|██        | 1/5 [02:42<10:49, 162.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.12525120788436395\n",
            "Val loss: 0.11471289593507261\n",
            "Val acc: 0.9608418174962291\n",
            "F1-Score: 0.9645239663041191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  40%|████      | 2/5 [05:26<08:08, 162.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.08970361092805466\n",
            "Val loss: 0.11236451434738495\n",
            "Val acc: 0.9452394419306183\n",
            "F1-Score: 0.9577609778338305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  60%|██████    | 3/5 [08:10<05:26, 163.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0696668062371471\n",
            "Val loss: 0.10214261710643768\n",
            "Val acc: 0.9543514328808447\n",
            "F1-Score: 0.9626316062827663\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  80%|████████  | 4/5 [10:55<02:43, 163.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.057400159469722115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 100%|██████████| 5/5 [13:37<00:00, 163.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss: 0.10701440099407644\n",
            "Val acc: 0.9507418929110104\n",
            "F1-Score: 0.9606747427001167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GbiWLXfzlCD"
      },
      "source": [
        "\n",
        "\n",
        "> Run for loading model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHzp4zhiCGkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad16ae1-8fe6-4c4e-eeec-832ccd0dcaef"
      },
      "source": [
        "model.load_state_dict(torch.load(model_path + \"/key_extractor\" + \"3e5_fully\" + \".pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us2AMAmag2_T"
      },
      "source": [
        "def accuracy_mean(preds, outs):\n",
        "  preds_flat = preds.flatten()\n",
        "  outs_flat = outs.flatten()\n",
        "  acc = 0\n",
        "  for tag in list(tag2idx.values()):\n",
        "    count = 0\n",
        "    tag_ids = [i for i, o in enumerate(outs_flat) if o == tag]\n",
        "    for i in tag_ids:\n",
        "      if preds_flat[i] == tag:\n",
        "        count += 1\n",
        "    acc += count / len(tag_ids)\n",
        "  return acc / len(tag2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex6bf2Y6G7QT"
      },
      "source": [
        "# Testing\n",
        "def test():\n",
        "  model.eval()\n",
        "  eval_loss, eval_acc, eval_acc_mean = 0, 0, 0\n",
        "  eval_steps = 0\n",
        "  preds, outs = [], []\n",
        "\n",
        "  for batch in test_loader:\n",
        "    batch = tuple(b.to(device) for b in batch)\n",
        "    b_inp_ids, b_mask, b_out_tgs = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      loss = model(b_inp_ids, token_type_ids=None, attention_mask=b_mask, labels=b_out_tgs)\n",
        "      logits = model(b_inp_ids, token_type_ids=None, attention_mask=b_mask)\n",
        "    \n",
        "    temp_preds = np.argmax(logits.cpu().numpy(), axis=2)\n",
        "    temp_outs = b_out_tgs.cpu().numpy()\n",
        "    preds.extend([list(p) for p in temp_preds])\n",
        "    outs.extend([list(o) for o in temp_outs])\n",
        "\n",
        "    acc = accuracy(temp_preds, temp_outs)\n",
        "    acc_mean = accuracy_mean(temp_preds, temp_outs)\n",
        "    \n",
        "    eval_loss += loss.item()\n",
        "    eval_acc += acc\n",
        "    eval_acc_mean += acc_mean\n",
        "    eval_steps += 1\n",
        "\n",
        "  eval_loss /= eval_steps\n",
        "  eval_acc /= eval_steps\n",
        "  eval_acc_mean /= eval_steps\n",
        "  print('Test loss:', eval_loss)\n",
        "  print('Test acc:', eval_acc)\n",
        "  print('Test acc mean:', eval_acc_mean)\n",
        "  print('F1-Score:', f1_score([oi for o in outs for oi in o], [pi for p in preds for pi in p], average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuIT_sXI0peg",
        "outputId": "a2f249ee-8a4a-4446-aa2d-1852b9a3321c"
      },
      "source": [
        "test() # for 3e5 model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.22427435251823158\n",
            "Test acc: 0.9762249601441711\n",
            "F1-Score: 0.964472719415991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXA5tj06RIe8",
        "outputId": "37d331b6-5170-4a84-ab1e-ba1b5cf33a4e"
      },
      "source": [
        "test() # for 3e5_fully model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.19158609764544723\n",
            "Test acc: 0.9507751091703067\n",
            "Test acc mean: 0.5708863405454127\n",
            "F1-Score: 0.95836161857404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_gZDjlKzyr3"
      },
      "source": [
        "def extract(sent):\n",
        "  tkns = tokenizer.tokenize(sent)\n",
        "  ids = torch.tensor([tokenizer.convert_tokens_to_ids(tkns)]).to(device)\n",
        "  mask = torch.tensor([[0] * len(tkns)]).to(device)\n",
        "  \n",
        "  model.eval()\n",
        "  logit = model(ids, token_type_ids=None, attention_mask=mask).detach().cpu().numpy()\n",
        "  pred = [list(p) for p in np.argmax(logit, axis=2)]\n",
        "  for i, tag in enumerate(pred[0]):\n",
        "    if tag == 2 or tag == 1:\n",
        "      print(tokenizer.convert_ids_to_tokens(ids[0].cpu().numpy())[i], tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKz_XHaSRneL",
        "outputId": "539ac1ae-ed20-4d5f-bca2-8ad9829a6bf5"
      },
      "source": [
        "extract(\"Evaluating Adaptive Resource Management for Distributed Real-Time Embedded Systems\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adaptive 2\n",
            "resource 1\n",
            "management 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}